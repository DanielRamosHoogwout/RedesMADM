---
title: "Tarea 2"
author: "Daniel Ramos, Jordi Vanrell, Sergi Fornes"
date: "17/11/2020"
output: pdf_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align="center", echo=TRUE, warning=FALSE, message=FALSE,autodep = TRUE,cache=FALSE)
library(knitr)
library(printr)
library(igraph)
```

We shall consider again the undirected Facebook friendship network considered in the last handout. The links in this network are contained in the file **facebook_sample_anon.txt**. Download it on your computer and upload it to R as a dataframe. Define an undirected graph with this list of edges.


**1)**  It has been observed in many networks an association between "centrality" and "lethality," defined as the fatal disconnection of the network when nodes are removed. Let's study this association on this network.

```{r}
data <- read.table("./data/facebook_sample_anon.txt")
head(data)
gf <- graph_from_data_frame(d=data, directed = F)
```


*a)* Repeat 1000 times the procedure of removing a random 0.1% of its set of nodes, and compute the average number of connected components of the resulting networks and the average fraction of the network represented by the largest component. Use **set.seed** to make your results reproducible.

```{r}
set.seed(2020)
longitud <- length(V(gf))#Numero de Nodos iniciales
new.gf <- gf #Copiamos gf
vector.connectedness <- double(1000) #Inicializamos vectores
vector.fraction.network <- double(1000)
for(x in 1:1000){
  #Escogemos nodos aleatorios
  nodos = sample(V(new.gf),round(0.001*longitud), replace = FALSE)
  length(V(new.gf))
  #Eliminamos los nodos escogidos anteriormente
  new.gf<- igraph::delete.vertices(new.gf, nodos)
  length(V(new.gf))
  vector.connectedness[x] <- components(new.gf)$no
  vector.fraction.network[x] <- max(components(new.gf)$csize)/sum(components(new.gf)$csize)
  
}
mean(vector.connectedness)
mean(vector.fraction.network)
```

*b)* Now, compute the number of connected components and the fraction represented by the largest component of the networks obtained after removing the most central 0.1% of nodes, for the following centrality indices (of course, if the most central 0.1% of nodes for two indices are the same set of nodes, you need not waste your time considering twice the same network): *degree*; *closeness*; *betweenness*; *page.rank*. (**Hint**: It might be convenient to define first a function that removes a given set of nodes of this graph and computes the number of connected components and the fraction represented by the largest component of the resulting network; then you will only need to apply it to the required different sets of most central nodes.) Is it what you expected? 

```{r}

V(gf)$degree <- degree(gf)
V(gf)$closeness <- closeness(gf)
V(gf)$betweenness <- betweenness(gf)

# Degree 
max_degree <- sort(V(gf)$degree, decreasing = TRUE)[1:4]
central_v_degree <- V(gf)[V(gf)$degree %in% max_degree]
# Eliminamos los nodos m치s centrales
n_gf <- delete_vertices(gf, central_v_degree)
components(n_gf)$no
max(components(n_gf)$csize) / sum(components(n_gf)$csize)

# Closeness:
V(gf)$closeness <- closeness(gf)
max_degree3 <- sort(V(gf)$closeness, decreasing = TRUE)[1:4]
central_v_degree3 <- V(gf)[V(gf)$closeness %in% max_degree3]
# Eliminamos los nodos m치s centrales
n_gf3 <- delete_vertices(gf, central_v_degree3)
components(n_gf3)$no
max(components(n_gf3)$csize) / sum(components(n_gf3)$csize)

# Betweenness:
V(gf)$betweenness <- betweenness(gf)
max_degree2 <- sort(V(gf)$betweenness, decreasing = TRUE)[1:4]
central_v_degree2 <- V(gf)[V(gf)$betweenness %in% max_degree2]
# Eliminamos los nodos m치s centrales
n_gf2 <- delete_vertices(gf, central_v_degree2)
components(n_gf2)$no
max(components(n_gf2)$csize) / sum(components(n_gf2)$csize)

# PageRank:
V(gf)$pageRank <- page_rank(gf,directed=F)$vector
vertices <- vector(,length(V(gf)))
for (x in 1:length(V(gf))){
  vertices[x] <- V(gf)$pageRank[[x]]
}
max_degree4 <- sort(vertices, decreasing = TRUE)[1:4]
central_v_degree4 <- V(gf)[V(gf)$pageRank %in% max_degree4]
# Eliminamos los nodos m치s centrales
n_gf4 <- delete_vertices(gf, central_v_degree4)
components(n_gf4)$no
max(components(n_gf4)$csize) / sum(components(n_gf4)$csize)

```

**2)** Now, consider the same graph as a directed one, and find the hubs and authorities scores. Compare with the page rank score. 

```{r}
gf_d <- graph_from_data_frame(d=data, directed = T)
hs <- hub_score(gf_d, weights=NA)$vector
as <- authority_score(gf, weights=NA)$vector

plot(gf_d, vertex.size=hs*50, main="Hubs")
plot(gf_d, vertex.size=as*30, main="Authorities")

library(networkR)
# data <- read.table("facebook_sample_anon.txt")
# head(data)
# gf_d <- graph_from_data_frame(d=data, directed = T)
kmax<-6 # 6 iteraciones
gf_d_mtx <- as_adjacency_matrix(gf_d)
op<-hits(gf_d_mtx,kmax)
op
str(op$authorities)
str(op$hubs)

```




